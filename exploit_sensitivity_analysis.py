# -*- coding: utf-8 -*-
"""
Created on January 2025

@author: Matthias Rohr
"""
# Import python modules
import os 
import seaborn as sns
import scipy
import matplotlib.pyplot as plt
import joblib
from joblib import Parallel, delayed
import numpy as np
import pandas as pd
import pickle
import time
import itertools
import matplotlib.patheffects as path_effects
from matplotlib.colors import TwoSlopeNorm

## Set working directory
wd = 'YOUR_PATH'

# Import function required to run the model :
import model_function as model    
import exploit_fun as exploit

os.chdir(wd)
#%%
# =============================================================================
# I. Simulation parameters
# =============================================================================

## Species parameters
S = 100          # Number of species
Ext_seed_rain = 1 #  External migration rate

## Landscape parameters
n = 50                # Size of the landscape grid
auto_corr = 5        # Autocorrelation of the environment across the landscape
structure = 'mosaic' # Structure of the environmental grid, can be: homogeneous, autocorelated ('mosaic'), random or gradient
env_range = [0, 1]   # Range of the environmental values

## Simulation processes parameters
phi = .5     # Relative importance of symmetric competition vs hierarchical competition
We = 1       # Strength of the environmental filtering
Wc = 10      # Strength of the competition
correl = 0   # Traits correlations
pool_sp = 16 # Number of species pool to generate for each parameter combination 
space_size = 50
## Parameters for model analysis
n_it = 2 # Number of randomization
n_sample = 8 # Number of subsamples for each observation scale
name = 'Sensi_analysis_phi05_corr0'
Trait_names = ['Symmetric_trait', 'Environmental_trait', 'Hierarchic_trait', 'Multivariate_metric']

# =============================================================================
# II. Load Simulation results
# =============================================================================
wd = 'C:\\Users\\rohrm\\Documents\\Th√®se LECA\\articles\\papier modele assemblage\\Codes\\sensi_analysis_res'
os.chdir(wd)

# Load Res sensi analysis
file_name = 'Results_' + name + '.pKl'
open_file = open(file_name, "rb")
Final_communities = pickle.load(open_file)
open_file.close()

# Load LHC parms
file_name = 'LHC_' + name + '.pKl'
open_file = open(file_name, "rb")
LHC = pickle.load(open_file)
open_file.close()

n_test = len(LHC) # Number of parameter combination tested
n_test = 10
# =============================================================================
# III. Compute the SES for each simulation
# =============================================================================

def par_sample_SES_sensi(k, results, traits_name, n_it, n_sample, K, S, space_size):
    """
    Compute SES, global, intra, and local diversities for each sample.

    Parameters:
    - k (int): Index of the sample.
    - Final_communities (dict): Dictionary containing final community matrices.
    - traits (array): Array of traits.
    - traits_name (list): List of trait names.
    - n_it (int): Number of iterations for the null model.
    - n_sample (int): Number of samples for each scale.
    - K (int): Carrying capacity.
    - S (int): Number of species.
    - space_size (int): Size of the space.

    Returns:
    - list: List containing SES, global, intra, and local diversities.
    """
    Final_community = results[k][0]
    trait = results[k][4]
    
    Obs_out = Final_community.reshape(-1, space_size**2).T
    sum_obs = np.sum(Obs_out, 0).reshape((1, S))
    loc_sample = model.local_sampling(space_size, S, Final_community, 10)
    
    intra_SES = exploit.scale_RAO(Obs_out, trait, traits_name, 'intra', n_it, n_sample, K, S)
    local_SES = exploit.scale_RAO(loc_sample, trait, traits_name, 'local', n_it, n_sample, K, S)
    global_SES = exploit.scale_RAO(sum_obs, trait, traits_name, 'global', n_it, n_sample, K, S)
    
    dtf_SES = pd.concat((intra_SES, local_SES, global_SES)) 
    
    Div_global = model.shannon_div(sum_obs / np.sum(sum_obs))
    Div_intra = model.shannon_div(Obs_out / np.sum(Obs_out, 1).reshape((len(Obs_out), 1)))
    Div_local = model.shannon_div(loc_sample / np.sum(loc_sample, 1).reshape((len(loc_sample), 1)))
    
    return list((dtf_SES, Div_global, Div_intra, Div_local))

def Out_dtf(results, Trait_names, n_it, n_sample, S, space_size, njobs):

    key = list(results.keys())
    
    Out_null_model = Parallel(n_jobs = njobs)(delayed(par_sample_SES_sensi)(k,
                                                                            results,
                                                                            Trait_names,
                                                                            n_it,
                                                                            n_sample,
                                                                            K,
                                                                            S,
                                                                            space_size) for k in key)
    # Extract SES data
    SES = [item[0] for item in Out_null_model]
    
    # Compute diversity dataframes for different scales
    Div_intra = exploit.dtf_div(Out_null_model, 1)
    Div_local = exploit.dtf_div(Out_null_model, 2)
    Div_global = exploit.dtf_div(Out_null_model, 3)
    
    return list((SES, Div_intra, Div_local, Div_global))

#%%

results = list()
njobs = joblib.cpu_count()
SES_dtf = pd.DataFrame()
start_time = time.time()
for s in range(n_test):
    K = int(LHC[s, 0])
    results.append(Out_dtf(Final_communities[s], Trait_names, n_it, n_sample, S, space_size, njobs))
    
    for i in range(len(results[s][0])):
    # for i in range(2):

        tempo = results[s][0][i]
        tempo['K'] = LHC[s, 0]
        tempo['mu'] = LHC[s, 1]
        tempo['Fec'] = LHC[s, 2]
        tempo['omega'] = LHC[s, 3]
        tempo['sigam_s'] = LHC[s, 4]
        tempo['sigma_h'] = LHC[s, 5]
        tempo['phi'] = phi
        tempo['corr'] = correl
        SES_dtf = pd.concat((SES_dtf, tempo))
    print("SES computing time: params combination" + str(s)  + "  %s minutes ---" % ((time.time() - start_time)/60))


save_name = "SES_" + name + '.pkl'
open_file = open(save_name, "wb")
pickle.dump(SES_dtf, open_file)
open_file.close()








































