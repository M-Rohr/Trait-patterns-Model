# -*- coding: utf-8 -*-
"""
Created on May 2024

@author: Matthias Rohr
"""
import os
import seaborn as sns
import scipy
import matplotlib.pyplot as plt
import joblib
from joblib import Parallel, delayed
import numpy as np
import pandas as pd
import pickle
import time
import itertools
import matplotlib.patheffects as path_effects

wd = 'C:\\Users\\rohrm\\Documents\\Th√®se LECA\\articles\\papier modele assemblage\\Codes'
os.chdir(wd)
# Import function required to run the model :
import model_function as model   

def load_data(simu):
    """
    Load data from picKle files related to simulation_experiment.py

    Parameters:
    - simu (str): The simulation name.
    
    Returns:
    - tuple: A tuple containing Final_communities, Traits, and Environment_matrix.
    """
    # Load final communities
    file_name = 'Results_' + simu + '.pKl'
    open_file = open(file_name, "rb")
    Final_communities = pickle.load(open_file)
    open_file.close()
    # Load traits
    file_name = 'Trait_' + simu + '.pKl'
    open_file = open(file_name, "rb")
    Traits = pickle.load(open_file)
    open_file.close()
    # Load environment matrix
    file_name = 'Environment_' + simu + '.pKl'
    open_file = open(file_name, "rb")
    Environment_matrix = pickle.load(open_file)
    open_file.close()
    
    return list((Final_communities, Traits, Environment_matrix))


def dtf_fun(SES, Traits_name):
    """
    Convert SES data into a DataFrame.

    Parameters:
    - SES (dict): SES data dictionary.
    - Traits_name (list): List of trait names.

    Returns:
    - array: Array containing SES data and corresponding trait names.
    """
    dtf = list()
    trait = list()
    for k in range(len(Traits_name)):
        dtf.append(SES[Traits_name[k]])
        trait.append(np.repeat(Traits_name[k], len(SES[Traits_name[k]])))
    dtf = np.hstack(dtf)
    trait = np.hstack(trait)
    out = np.vstack((dtf, trait)).T
    return out


def SES_calc(obs_RAO, rnd_RAO, traits_name):
    """
    Calculate standardized effect size (SES) and rank for each trait.

    Parameters:
    - obs_RAO (dict): Dictionary containing observed RAO values for each trait.
    - rnd_RAO (dict): Dictionary containing randomized RAO values for each trait.
    - traits_name (list): List of trait names.

    Returns:
    - list: List containing dictionaries of SES and rank for each trait.
    """
    SES = dict()
    rank = dict()
    for t in traits_name:
        SES[t] = (obs_RAO[t] - np.mean(rnd_RAO[t], 1)) / np.std(rnd_RAO[t], 1)
        rank[t] = np.sum(rnd_RAO[t] < obs_RAO[t][:, np.newaxis], 1)
    return list((SES, rank))


def scale_RAO(community_matrix, traits, Traits_name, scale, n_it, n_sample, K, S):
    """
    Compute SES and rank for the given scale.

    Parameters:
    - community_matrix (array): Community matrix with species as columns and sites as rows.
    - traits (array): Array of traits, with the different traits as columns and speceis as row
    - Traits_name (list): List of trait names.
    - scale (str): Scale of the analysis (intra, local, or global).
    - n_it (int): Number of iterations for the null model.
    - n_sample (int): Number of samples for each scale.
    - K (int): Carrying capacity.
    - S (int): Number of species.

    Returns:
    - DataFrame: DataFrame containing SES and rank values.
    """
    # get the sampled community matrix and its randomized mirror
    if scale == 'intra':
        sample_site = np.random.choice(range(len(community_matrix[:,0])), n_sample, replace = False)
        obs_samp = community_matrix[sample_site, :]
        rnd_samp = model.EQ_R_cst_ric(community_matrix[sample_site, :], n_it)
    else:    
        obs_samp, rnd_samp = model.obs_rnd_sampling(community_matrix, n_sample, n_it, S, K)
    # Compute the RAO diversity for all obs and random samples
    obs_RAO = dict()
    rnd_RAO =  dict()
    for t in range(len(Traits_name)):
        if t < 3:
            obs_RAO[Traits_name[t]] = model.RAO(obs_samp, traits[:, t].reshape((S,1)))
        if t == 3:
            obs_RAO[Traits_name[t]] = model.RAO(obs_samp, traits)

        rnd_par_RAO = list()
        for i in range(n_it):
            if t < 3:
                rnd_par_RAO.append(model.RAO(rnd_samp[i, :, :], traits[:, t].reshape((S,1))))
            if t == 3:
                rnd_par_RAO.append(model.RAO(rnd_samp[i, :, :], traits))


        rnd_RAO[Traits_name[t]] = np.vstack(rnd_par_RAO).T
        
    SES, rank = SES_calc(obs_RAO, rnd_RAO, Traits_name)
    SES = dtf_fun(SES, Traits_name)
    rank = dtf_fun(rank, Traits_name)
    dtf_SES = pd.DataFrame(SES, columns=['SES', 'Traits'])
    dtf_SES['SES'] = SES[:,0].astype('float32')
    dtf_SES['scale'] = scale
    dtf_SES['rank'] = rank[:,0].astype('float32')

    return dtf_SES



def par_sample_SES(k, Final_communities, traits, traits_name, n_it, n_sample, K, S, space_size):
    """
    Compute SES, global, intra, and local diversities for each sample.

    Parameters:
    - k (int): Index of the sample.
    - Final_communities (dict): Dictionary containing final community matrices.
    - traits (array): Array of traits.
    - traits_name (list): List of trait names.
    - n_it (int): Number of iterations for the null model.
    - n_sample (int): Number of samples for each scale.
    - K (int): Carrying capacity.
    - S (int): Number of species.
    - space_size (int): Size of the space.

    Returns:
    - list: List containing SES, global, intra, and local diversities.
    """
    Final_community = Final_communities[k]
    trait = traits[k]
    
    Obs_out = Final_community.reshape(-1, space_size**2).T
    sum_obs = np.sum(Obs_out, 0).reshape((1, S))
    loc_sample = model.local_sampling(space_size, S, Final_community, 10)
    
    intra_SES = scale_RAO(Obs_out, trait, traits_name, 'intra', n_it, n_sample, K, S)
    local_SES = scale_RAO(loc_sample, trait, traits_name, 'local', n_it, n_sample, K, S)
    global_SES = scale_RAO(sum_obs, trait, traits_name, 'global', n_it, n_sample, K, S)
    
    dtf_SES = pd.concat((intra_SES, local_SES, global_SES)) 
    
    Div_global = model.shannon_div(sum_obs / np.sum(sum_obs))
    Div_intra = model.shannon_div(Obs_out / np.sum(Obs_out, 1).reshape((len(Obs_out), 1)))
    Div_local = model.shannon_div(loc_sample / np.sum(loc_sample, 1).reshape((len(loc_sample), 1)))
    
    return list((dtf_SES, Div_global, Div_intra, Div_local))

def dtf_div(Out, metric):
    """
    Combine diversity metrics from multiple samples into a single DataFrame.

    Parameters:
    - Out (list): List containing diversity metrics for each sample.
    - metric (int): Index of the diversity metric.

    Returns:
    - numpy.ndarray: Combined diversity metrics.
    """
    dtf = list()
    for k in range(len(Out)):
        dtf.append(Out[k][metric])
    dtf = np.hstack(dtf)
    return dtf


def Out_dtf(name, n, S, n_it, n_sample, Traits_name, K, njobs):
    """
    Generate SES dataframes for different scales (intra, local, global) and corresponding diversity metrics.

    Parameters:
    - name (str): The simulation identifier.
    - n (int): Size of the landscape grid.
    - S (int): Number of species.
    - n_it (int): Number of randomization iterations for the null models.
    - n_sample (int): Number of samples for each scale.
    - Traits_name (list): List of trait names.
    - K (int): Carrying capacity of the cells.

    Returns:
    - tuple: A tuple containing SES dataframes for different scales (intra, local, global) and corresponding diversity dataframes.
    """
    # Load data
    Final_communities, trait, Env_matrix = load_data(name)
    key = list(Final_communities.keys())
    
    # Run parallel computation for null models
    with np.errstate(divide='ignore'):
        Out_null_model = Parallel(n_jobs = njobs)(delayed(par_sample_SES)(k,
                                                                      Final_communities,
                                                                      trait,
                                                                      Traits_name,
                                                                      n_it,
                                                                      n_sample,
                                                                      K,
                                                                      S,
                                                                      n) for k in key)
        

        # Extract SES data
        SES = [item[0] for item in Out_null_model]
        
        # Compute diversity dataframes for different scales
        Div_intra = dtf_div(Out_null_model, 1)
        Div_local = dtf_div(Out_null_model, 2)
        Div_global = dtf_div(Out_null_model, 3)
        
    return list((SES, Div_intra, Div_local, Div_global))

























